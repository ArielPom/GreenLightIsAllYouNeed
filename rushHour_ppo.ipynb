{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# install relevant packages\n",
    "INSTALL_PACKAGES = False\n",
    "\n",
    "if INSTALL_PACKAGES:\n",
    "    !pip install -q traci\n",
    "    print(\"installed traci\")\n",
    "    !add-apt-repository ppa:sumo/stable -y > /dev/null\n",
    "    print(\"add apt\")\n",
    "    !apt-get update -y > /dev/null\n",
    "    print(\"apt update\")\n",
    "    !apt-get install -y sumo sumo-tools sumo-doc > /dev/null\n",
    "    print(\"installed sumo\")\n",
    "\n",
    "    !pip install ipywidgets\n",
    "    print(\"installed ipywidgets\")\n",
    "    \n",
    "    os.environ['SUMO_HOME'] = \"/usr/share/sumo/\"\n",
    "\n",
    "print(\"finished installing packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import traci\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from traffic_generator import TrafficGenerator\n",
    "from episodes_data import EpisodesData\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import csv\n",
    "\n",
    "# set the precision for numpy output\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"finished importing packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation configuration\n",
    "\n",
    "# files\n",
    "CONFIG_FILE = \"./sumo_files/s2.sumocfg\"\n",
    "GUI_SETTINGS_FILE = \"./sumo_files/viewsettings.xml\"\n",
    "ROU_FILE = \"./sumo_files/s2.rou.xml\"\n",
    "\n",
    "# simulation command\n",
    "SUMO_HEADLESS = \"sumo\"\n",
    "SUMO_GUI = \"sumo-gui\"\n",
    "SUMO_APP = SUMO_HEADLESS\n",
    "STEP_LENGTH = \"1\"   # Simulation step length in seconds\n",
    "TRAIN_MODEL_TIME_STEP = 4 # train the model every x seconds\n",
    "TRAIN_MODEL_STEPS_SIZE = int(TRAIN_MODEL_TIME_STEP / float(STEP_LENGTH))\n",
    "DELAY = \"0\"       # Simulation delay in seconds, used for gui mode\n",
    "DEBUG = False\n",
    "EPISODES_GUI = 200\n",
    "\n",
    "# simulation parameters\n",
    "STATE_SIZE = (4, 10, 5) # 4 lanes, 10 vehicles, 5 features (distance, speed, waiting_time, traffic_light_phase, traffic_light_duration)\n",
    "INPUT_SIZE = np.prod(STATE_SIZE)\n",
    "ACTION_SIZE = 2\n",
    "MAX_STATE_SIZE = (4, 100, 5)\n",
    "EPISODES = 200\n",
    "MAX_SIM_TIME = 1000\n",
    "MAX_STEPS = MAX_SIM_TIME / float(STEP_LENGTH)\n",
    "N_CARS = 200\n",
    "\n",
    "print(\"finished setting up simulation configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the simulation traffic\n",
    "\n",
    "gen = TrafficGenerator(ROU_FILE, MAX_SIM_TIME, float(STEP_LENGTH), int(N_CARS / 2), seed, distribution=\"uniform\")\n",
    "gen.generate_routefile(plot_distribution=True)\n",
    "\n",
    "gen = TrafficGenerator(ROU_FILE, MAX_SIM_TIME, float(STEP_LENGTH), N_CARS, seed, distribution=\"normal\")\n",
    "gen.generate_routefile(plot_distribution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, input): return mish(input)\n",
    "\n",
    "# Actor module, categorical actions only\n",
    "class Actor_mish(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            activation(),\n",
    "            nn.Linear(256, 128),\n",
    "            activation(),\n",
    "            nn.Linear(128, n_actions),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "# Critic module\n",
    "class Critic_mish(nn.Module):\n",
    "    def __init__(self, state_dim, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            activation(),\n",
    "            nn.Linear(256, 128),\n",
    "            activation(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    def __init__(self):\n",
    "        self.vehicles_info = {}\n",
    "        self.arrived_vehicles = 0\n",
    "        self.total_waiting_time = 0\n",
    "        self.avg_speed = 0\n",
    "        self.epsilon = 0.2\n",
    "        self.current_green_lane = None\n",
    "        self.prev_green_lane = None\n",
    "        self.prev_green_lane_duration = None\n",
    "\n",
    "    def start(self, episode):\n",
    "        try:\n",
    "            \n",
    "            if episode == 0: # human mode\n",
    "                SUMO_APP = SUMO_GUI\n",
    "                DELAY = \"200\"\n",
    "            elif episode % EPISODES_GUI == 0:\n",
    "                SUMO_APP = SUMO_GUI\n",
    "                DELAY = \"100\"\n",
    "            else:\n",
    "                SUMO_APP = SUMO_HEADLESS\n",
    "                DELAY = \"0\"\n",
    "\n",
    "            SUMO_CMD = [SUMO_APP, \"-c\", CONFIG_FILE, \"-g\", GUI_SETTINGS_FILE, \"--step-length\", STEP_LENGTH ,\"--delay\", DELAY, \"--time-to-teleport\" , \"-1\", \"--start\", \"--quit-on-end\", \"--keep-after-arrival\", str(MAX_SIM_TIME) ]\n",
    "            traci.start(SUMO_CMD)\n",
    "        except Exception as e:\n",
    "            print(f\"Error starting simulation: {e}\")\n",
    "            traci.close()\n",
    "\n",
    "    def close(self):\n",
    "        traci.close()\n",
    "\n",
    "    def calc_steps_to_advance(self, action_changed_phase):\n",
    "        if action_changed_phase:\n",
    "            time_to_step = 4 # 3 yellow + 1 green\n",
    "        else:\n",
    "            time_to_step = 1\n",
    "            \n",
    "        n_steps = int(time_to_step / float(STEP_LENGTH))\n",
    "        return n_steps\n",
    "    \n",
    "    def advance(self, n_steps=1):\n",
    "        for _ in range(n_steps):\n",
    "            traci.simulationStep()\n",
    "            self.arrived_vehicles += traci.simulation.getArrivedNumber()\n",
    "            # store the vehicles info\n",
    "            for vehicle_id in traci.vehicle.getIDList():\n",
    "                distance_travelled = traci.vehicle.getDistance(vehicle_id)\n",
    "                time_alive = traci.simulation.getTime() - traci.vehicle.getDeparture(vehicle_id)\n",
    "                waiting_time = traci.vehicle.getAccumulatedWaitingTime(vehicle_id)\n",
    "                self.vehicles_info[vehicle_id] = [distance_travelled, time_alive, waiting_time]\n",
    "                \n",
    "    def get_state(self):\n",
    "        lane_counter = [0, 0, 0, 0]\n",
    "        state = np.zeros(MAX_STATE_SIZE)\n",
    "        for vehicle_id in traci.vehicle.getIDList():\n",
    "            upcoming_tls = traci.vehicle.getNextTLS(vehicle_id)\n",
    "            if len(upcoming_tls) != 0:\n",
    "                speed = traci.vehicle.getSpeed(vehicle_id)\n",
    "                waiting_time = traci.vehicle.getAccumulatedWaitingTime(vehicle_id)\n",
    "                tl_id, tl_index, distance, tl_state = upcoming_tls[0]\n",
    "                vehicle_state_index = lane_counter[tl_index]\n",
    "                state[tl_index][vehicle_state_index] = [distance, speed, waiting_time, 0, 0]\n",
    "                lane_counter[tl_index] += 1\n",
    "\n",
    "        tl_id = traci.trafficlight.getIDList()[0]\n",
    "        current_phase = traci.trafficlight.getPhase(tl_id)\n",
    "        phase_duration = traci.trafficlight.getSpentDuration(tl_id)\n",
    "        if (current_phase == 0 or current_phase == 3): # 0 has green or about to turn green\n",
    "            green_lane = 0\n",
    "            state[[0,2],:,3:] = [1, phase_duration]\n",
    "            state[[1,3],:,3:] = [0, phase_duration]\n",
    "        else:\n",
    "            green_lane = 1\n",
    "            state[[0,2],:,3:] = [0, phase_duration]\n",
    "            state[[1,3],:,3:] = [1, phase_duration]\n",
    "\n",
    "        # sort by distance, take only the first vehicles and flatten the state STATE_SIZE -> INPUT_SIZE\n",
    "        state = np.round(np.flip(np.sort(state, axis=1), axis=1), 1)\n",
    "        state = state[:, :STATE_SIZE[1], :]\n",
    "        state = state.flatten()\n",
    "        return torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    \n",
    "    def calc_reward(self):\n",
    "        # choose the reward policy \n",
    "        reward_policy = \"average_speed\" # \"waiting_time\" or \"average_speed\" or \"mixed\"\n",
    "        reward_alpha = 0.5\n",
    "        \n",
    "        # init parameters\n",
    "        total_cars_in_simulation = traci.vehicle.getIDCount()\n",
    "        car_speeds = []\n",
    "        waiting_times = []\n",
    "\n",
    "        # calculate the reward\n",
    "        for vehicle_id in traci.vehicle.getIDList():\n",
    "            car_speeds.append(traci.vehicle.getSpeed(vehicle_id))\n",
    "            if traci.vehicle.getNextTLS(vehicle_id):\n",
    "                waiting_times.append(traci.vehicle.getAccumulatedWaitingTime(vehicle_id))\n",
    "\n",
    "        \n",
    "        total_speed = sum(car_speeds)\n",
    "        total_waiting_time = sum(waiting_times)\n",
    "\n",
    "        avg_speed = 0 if len(car_speeds) == 0 else (total_speed / len(car_speeds))\n",
    "        avg_waiting_time = 0 if len(waiting_times) == 0 else (total_waiting_time / len(waiting_times))\n",
    "\n",
    "\n",
    "        # calculate the reward based on the reward policy\n",
    "        if reward_policy == \"waiting_time\":\n",
    "            reward = -avg_waiting_time\n",
    "        elif reward_policy == \"average_speed\":\n",
    "            reward = avg_speed\n",
    "        elif reward_policy == \"mixed\":\n",
    "            reward = (reward_alpha * avg_speed) - ((1 - reward_alpha) * avg_waiting_time)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "    def perform_action(self, action):\n",
    "\n",
    "        tl_id = traci.trafficlight.getIDList()[0]\n",
    "        current_phase = traci.trafficlight.getPhase(tl_id)\n",
    "        if (current_phase == 0 or current_phase == 3): # 0 has green or about to turn green\n",
    "            current_green_lane = 0\n",
    "        else:\n",
    "            current_green_lane = 1\n",
    "\n",
    "        switch_green_lane = (current_green_lane != action)\n",
    "\n",
    "        if switch_green_lane:\n",
    "            if current_green_lane == 0:\n",
    "                new_phase = 1 # turn to yellow for lane 1\n",
    "            else:\n",
    "                new_phase = 3 # turn to yellow for lane 0\n",
    "        else:\n",
    "            new_phase = current_phase\n",
    "\n",
    "        self.prev_green_lane = current_green_lane\n",
    "        self.prev_green_lane_duration = traci.trafficlight.getSpentDuration(tl_id)\n",
    "        self.current_green_lane = action\n",
    "\n",
    "        traci.trafficlight.setPhase(tl_id, new_phase) # set again the same phase (to reset the duration)\n",
    "        return switch_green_lane\n",
    "    \n",
    "    def print_results(self, episode):\n",
    "        \n",
    "        total_vehicles = self.arrived_vehicles\n",
    "        total_sim_time = round(traci.simulation.getTime(), 1)\n",
    "        vehicles_per_minute = round(total_vehicles / total_sim_time * 60, 2)\n",
    "        total_waiting_time = sum(self.vehicles_info[vehicle_id][2] for vehicle_id in self.vehicles_info)\n",
    "\n",
    "        # calc average speed of all cars (distance / time_alive)\n",
    "        for vehicle_id in self.vehicles_info:\n",
    "            dist = self.vehicles_info[vehicle_id][0]\n",
    "            time_alive = self.vehicles_info[vehicle_id][1]\n",
    "            avg_speed = dist / time_alive\n",
    "            \n",
    "        total_accumulated_avg_speed = sum(self.vehicles_info[vehicle_id][0] / self.vehicles_info[vehicle_id][1] for vehicle_id in self.vehicles_info)\n",
    "        avg_speed = 0 if total_vehicles == 0 else (total_accumulated_avg_speed / total_vehicles)\n",
    "        \n",
    "        # store the results for plotting\n",
    "        self.total_waiting_time = total_waiting_time\n",
    "        self.avg_speed = avg_speed\n",
    "\n",
    "        print(f\"Episode {episode}: {total_vehicles} vehicles, {total_sim_time} seconds ({vehicles_per_minute} vehicles/min), total waiting time: {total_waiting_time}, average speed: {avg_speed:.2f}\")\n",
    "\n",
    "print(\"finished defining the simulation class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_mish = Actor_mish(INPUT_SIZE, ACTION_SIZE, activation=Mish)\n",
    "critic_mish = Critic_mish(INPUT_SIZE, activation=Mish)\n",
    "\n",
    "actor = actor_mish\n",
    "critic = critic_mish\n",
    "\n",
    "# define hyperparameters\n",
    "actor_adam_optimizer = optim.Adam(actor.parameters(), lr=1e-5) # optimizer for the actor\n",
    "critic_adam_optimizer = optim.Adam(critic.parameters(), lr=1e-4) # optimizer for the critic\n",
    "gamma = 0.99 # discount factor\n",
    "exploration_epsilon = 0.2\n",
    "clipping_epsilon = 0.1\n",
    "loss_fn = nn.MSELoss() # loss function\n",
    "\n",
    "print(\"finished creating the agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entopry_coef = 0.1\n",
    "\n",
    "def policy_loss(old_log_prob, log_prob, entropy, advantage, eps):\n",
    "    ratio = (log_prob - old_log_prob).exp()\n",
    "    clipped = torch.clamp(ratio, 1-eps, 1+eps)*advantage\n",
    "    \n",
    "    min = torch.min(ratio*advantage, clipped)\n",
    "    entropy_bonus = (entopry_coef * entropy).mean()\n",
    "    loss = - (min + entropy_bonus)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script echo \"Skip the training loop\"\n",
    "\n",
    "# actor.load_state_dict(torch.load(\"model_checkpoints/actor_weights_100.pth\"))\n",
    "# critic.load_state_dict(torch.load(\"model_checkpoints/critic_weights_100.pth\"))\n",
    "\n",
    "# store for each action all its probabilities\n",
    "# file = open('probs_history.csv', 'w', newline='')\n",
    "# writer = csv.writer(file)\n",
    "# writer.writerow(['episode', 'action', 'step', 'prob0', 'prob1'])\n",
    "\n",
    "episodes_data = EpisodesData()\n",
    "for episode in tqdm(range(1, EPISODES + 1), desc=\"Training Episodes\"):\n",
    "    tqdm.write(f\"Starting episode {episode}\")\n",
    "    \n",
    "    simulation = Simulation()\n",
    "    simulation.start(episode)\n",
    "\n",
    "    prev_prob_act_log = 0\n",
    "    action = 0\n",
    "    state = np.zeros(STATE_SIZE).flatten()\n",
    "    state = torch.tensor(state ,dtype=torch.float32).unsqueeze(0)\n",
    "    DEBUG = True if episode % EPISODES_GUI == 0 else False\n",
    "\n",
    "    step = 0\n",
    "    while traci.simulation.getMinExpectedNumber() > 0 and step < MAX_STEPS:\n",
    "\n",
    "        any_cars_in_simulation = traci.vehicle.getIDCount()\n",
    "        training_step = any_cars_in_simulation\n",
    "\n",
    "        if (training_step):\n",
    "            random_action = random.random() < exploration_epsilon\n",
    "            if random_action: # exploration move\n",
    "                action = torch.randint(0, ACTION_SIZE, (1,))\n",
    "                probs = torch.ones(ACTION_SIZE) / ACTION_SIZE\n",
    "                prob_act_log = torch.log(torch.tensor(1.0 / ACTION_SIZE, requires_grad=True))\n",
    "                entropy = torch.tensor(1.0 / ACTION_SIZE) * torch.log(torch.tensor(1.0 / ACTION_SIZE))\n",
    "            else: # use the actor to predict the action\n",
    "                probs = actor(state)\n",
    "                dist = torch.distributions.Categorical(probs = probs)\n",
    "                action = dist.sample()\n",
    "                prob_act_log = dist.log_prob(action)\n",
    "                entropy = dist.entropy()\n",
    "                if episode % 10 == 0:\n",
    "                    probs_list = probs.detach().numpy()[0]\n",
    "                    writer.writerow([episode, action.item(), step, probs_list[0], probs_list[1]])\n",
    "\n",
    "            action = action.item()\n",
    "\n",
    "        # advance the simulation\n",
    "        action_changed_phase = simulation.perform_action(action)\n",
    "        n_steps = simulation.calc_steps_to_advance(action_changed_phase)\n",
    "        simulation.advance(n_steps)\n",
    "        step += n_steps\n",
    "\n",
    "        if (training_step):\n",
    "            # get the next state and calculate the reward\n",
    "            next_state = simulation.get_state()\n",
    "            reward = simulation.calc_reward()\n",
    "            critic_state = critic(state)\n",
    "            critic_next_state = critic(next_state)\n",
    "            advantage = reward + (gamma * critic_next_state - critic_state)\n",
    "            prev_state = state\n",
    "            state = next_state\n",
    "\n",
    "            if prev_prob_act_log :\n",
    "                # train the actor if the action taken was not a random exploration action\n",
    "                if True:\n",
    "                    actor_loss = policy_loss(prev_prob_act_log.detach(), prob_act_log, entropy, advantage.detach(), clipping_epsilon)\n",
    "                    actor_adam_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_adam_optimizer.step()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        probs = actor(prev_state)\n",
    "\n",
    "                # train the critic\n",
    "                critic_loss = advantage.abs().mean()\n",
    "                critic_adam_optimizer.zero_grad()   \n",
    "                critic_loss.backward()\n",
    "                critic_adam_optimizer.step()\n",
    "\n",
    "            prev_prob_act_log = prob_act_log       \n",
    "\n",
    "    print(f\"Episode {episode} finished\\n\")\n",
    "    \n",
    "    if (episode % 10 == 0):\n",
    "        torch.save(actor.state_dict(), f\"model_checkpoints/actor_weights_{episode}_mixed.pth\")\n",
    "        torch.save(critic.state_dict(), f\"model_checkpoints/critic_weights_{episode}_mixed.pth\")\n",
    "\n",
    "    simulation.print_results(episode)\n",
    "    episodes_data.add_episode_data(simulation)\n",
    "    simulation.close()\n",
    "\n",
    "episodes_data.plot_training_results()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo \"Skip the testing loop\"\n",
    "\n",
    "episodes_data = EpisodesData()\n",
    "\n",
    "# Load the model in evaluation mode\n",
    "actor.load_state_dict(torch.load(\"model_checkpoints/actor_weights_40.pth\"))\n",
    "actor.eval()\n",
    "\n",
    "SUMO_APP = SUMO_GUI # SUMO_GUI or SUMO_HEADLESS\n",
    "DELAY = \"1000\"\n",
    "SUMO_CMD = [SUMO_APP, \"-c\", CONFIG_FILE, \"-g\", GUI_SETTINGS_FILE, \"--step-length\", STEP_LENGTH ,\"--delay\", DELAY, \"--start\", \"--quit-on-end\"]\n",
    "\n",
    "simulation = Simulation()\n",
    "simulation.start(0)\n",
    "\n",
    "state = np.zeros(STATE_SIZE).flatten()\n",
    "state = torch.tensor(state ,dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "step = 0\n",
    "while traci.simulation.getMinExpectedNumber() > 0 and step < MAX_STEPS:\n",
    "\n",
    "    if (step % TRAIN_MODEL_STEPS_SIZE == 0):\n",
    "        with torch.no_grad():\n",
    "            # Use the actor to predict action probabilities\n",
    "            probs = actor(state)\n",
    "            dist = torch.distributions.Categorical(probs = probs)\n",
    "            action = dist.sample()\n",
    "            prob_act_log = dist.log_prob(action)\n",
    "            action = action.item()\n",
    "\n",
    "            \n",
    "            # perform the action\n",
    "            simulation.perform_action(action)\n",
    "    \n",
    "    # advance the simulation\n",
    "    traci.simulationStep()\n",
    "    next_state = simulation.get_state()\n",
    "\n",
    "    if (step % TRAIN_MODEL_STEPS_SIZE == 0):\n",
    "        state = next_state\n",
    "\n",
    "    step += 1\n",
    "\n",
    "simulation.print_results(episode)\n",
    "episodes_data.add_episode_data(simulation)\n",
    "simulation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo \"Skip the simulation without the model\"\n",
    "\n",
    "# run the simulation without the model\n",
    "\n",
    "simulation = Simulation()\n",
    "simulation.start(0)\n",
    "\n",
    "step = 0\n",
    "while traci.simulation.getMinExpectedNumber() > 0 and step < MAX_STEPS:\n",
    "    simulation.advance(1)\n",
    "    step += 1\n",
    "\n",
    "simulation.print_results(0)\n",
    "simulation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo \"Skip the human controlled simulation\"\n",
    "\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "user_input = threading.Event()\n",
    "stop_input_thread = False\n",
    "\n",
    "def wait_for_input():\n",
    "    global stop_input_thread\n",
    "    while not stop_input_thread:\n",
    "        input(\"Press Enter to switch the lights phase\")\n",
    "        user_input.set()\n",
    "\n",
    "# Start a separate thread to handle user input\n",
    "human_control_thread = threading.Thread(target=wait_for_input, daemon=True)\n",
    "human_control_thread.start()\n",
    "\n",
    "Simulation = Simulation()\n",
    "Simulation.start(0)\n",
    "\n",
    "# add arrow image to the simulation\n",
    "img_id = \"arrow_img\"\n",
    "traci.poi.add(img_id, 50, 50, (255, 0, 0, 255), layer=340, width=20, height=20, imgFile=\"right_left.png\")\n",
    "\n",
    "tl_id = traci.trafficlight.getIDList()[0]\n",
    "step = 0\n",
    "while traci.simulation.getMinExpectedNumber() > 0 and step < MAX_STEPS:\n",
    "    Simulation.advance(1)\n",
    "    step += 1\n",
    "    if user_input.is_set():\n",
    "        # switch green lane and update the arrow image\n",
    "        current_phase = traci.trafficlight.getPhase(tl_id)\n",
    "        next_phases = (current_phase + 1) % 4\n",
    "        traci.trafficlight.setPhase(tl_id, next_phases)\n",
    "        img_file = \"up_down.png\" if next_phases == 0 or next_phases == 3 else \"right_left.png\"\n",
    "        traci.poi.remove(img_id)\n",
    "        traci.poi.add(img_id, 50, 50, (255, 0, 0, 255), layer=340, width=20, height=20, imgFile=img_file)\n",
    "        user_input.clear()\n",
    "           \n",
    "Simulation.print_results(0)\n",
    "Simulation.close()\n",
    "\n",
    "# Stop the input thread\n",
    "stop_input_thread = True\n",
    "human_control_thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
